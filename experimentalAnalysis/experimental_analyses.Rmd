---
title: "Experimental analyses for Toyokawa & Gaissmaier (2021)"
author: "Wataru Toyokawa"
date: "2021/2/17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = TRUE, 
  warning=FALSE, 
  error=FALSE
  )
```

```{r, include=FALSE, echo = TRUE, fig.show='hold'}
library(tidyverse)
library(cowplot)
library(knitr)
library(ggpubr)
setwd("~/Dropbox/wataru/papers/RiskySocialLearning/experiment/overall_analysis")
source("functions.R")
```

# The 2-armed bandit session

## The task
The optimal risky option produces either 50 or 550 points at probability 0.7 and 0.3, respectively (the expected payoff was 200). The safe option produced 150 points (with a small Gaussian noise with s.d. = 5).

## Data
```{r 2-armed-data}
behaviour_main_0820 <- read.csv("behaviour_main_0820.csv")
behaviour_indiv_0820 <-read.csv("behaviour_indiv_0820.csv")
allBehaviour0820 <- rbind(behaviour_main_0820, behaviour_indiv_0820)
allBehaviour0820 <- allBehaviour0820 %>% 
  dplyr::filter(amazonID != 'INHOUSETEST2') %>%  # eliminating data generated by debug tests
  dplyr::filter(amazonID != '5eac70db94edd22d57fa00c4') # a bug in the data storaging process

# make the choice data binary
allBehaviour0820$choice_num = NA
allBehaviour0820$choice_num[which(allBehaviour0820$choice=='sure')] = 0
allBehaviour0820$choice_num[which(allBehaviour0820$choice=='risky')] = 1
allBehaviour0820$choice_num[which(allBehaviour0820$choice=='miss')] = -1

allBehaviour0820$riskDistributionId_factor = 'Condition 5'
allBehaviour0820$riskDistributionId_factor[which(allBehaviour0820$riskDistributionId==6)] = 'Condition 6'
allBehaviour0820$riskDistributionId_factor[which(allBehaviour0820$riskDistributionId==7)] = 'Condition 7'

allBehaviour0820$indivOrGroup_factor = 'Individual'
allBehaviour0820$indivOrGroup_factor[allBehaviour0820$indivOrGroup == 1] = 'Social'

allBehaviour0820$groupSize_category = 'Small'
allBehaviour0820$groupSize_category[which(allBehaviour0820$groupSize==1)] = 'Individual'
allBehaviour0820$groupSize_category[which(allBehaviour0820$groupSize>4)] = 'Large'
allBehaviour0820$groupSize_category = factor(allBehaviour0820$groupSize_category, levels = c('Individual','Small','Large'))

# summarised data
allBehaviour0820_summarised = allBehaviour0820 %>%
	group_by(amazonID,room,indivOrGroup,exp_condition,riskDistributionId_factor,groupSize_category) %>%
	summarise(n = n(), averageRiskyChoice = mean(choice_num, na.rm=TRUE), groupSize = mean(groupSize,na.rm=TRUE))
allBehaviour0820_summarised$groupID = allBehaviour0820_summarised$room
allBehaviour0820_summarised$groupID[which(allBehaviour0820_summarised$indivOrGroup==0)] = 'Individual'

# Number of subjects
allBehaviour0820_summarised %>% dplyr::filter(n > 35) %>% 
  .$indivOrGroup %>% 
  table()

# Nnmber of groups for different group sizes
allBehaviour0820_summarised %>% dplyr::filter(n > 35) %>% 
  group_by(room) %>% 
  summarise(groupSize = median(ceiling(groupSize), na.rm=TRUE)) %>% 
  .$groupSize %>% 
  table()
```

## Model fitting

This section contains code for the model-fitting. Fitting should take quite a long time, so you can skip this chunk and the next (note: these chunks are not evaluated at the moment. Change to "eval=TRUE" if you fancy making them active). Instead, the csv data loaded in the two chunks below contains estimation values from which the figures of the paper can be reproduced. 

### Individual condition 
```{r eval = FALSE}
# debug
chains = 4
parallel_chains = 4
thin = 1
iter_warmup = 40
iter_sampling = 80
# mcmc params (the followings were used in the fitting analysis)
# chains <- 6
# parallel_chains <- 6
# thin <- 4
# iter_warmup <- 2000
# iter_sampling <- 6000

completedIDs = which(table(allBehaviour0820$amazonID) >= 36) %>% names()
allBehaviour0820_indiv = allBehaviour0820 %>% dplyr::filter(amazonID %in% completedIDs) %>%
  dplyr::filter(indivOrGroup == 0) # note this is only the individual condition
allBehaviour0820_indiv$sub = as.numeric(as.factor(allBehaviour0820_indiv$amazonID))
allBehaviour0820_indiv = allBehaviour0820_indiv %>% group_by(amazonID) %>% arrange(round, .by_group = TRUE)
# preparing data fed to Stan
ReinforcementLearningStanData_indiv_0820 = list(
    All = nrow(allBehaviour0820_indiv),
    Nsub = length(table(allBehaviour0820_indiv$amazonID)),
    Ncue = 2, # number of options
    Ntrial = 70,
    Ncon = 1,
    sub = allBehaviour0820_indiv$sub,
    Y = allBehaviour0820_indiv$choice_num + 1,
    trial = allBehaviour0820_indiv$round,
    payoff = allBehaviour0820_indiv$payoff / 100
    )

# # Basic reinforcement learning model
stan_file_AL00_multiVar_LKJ <- file.path(this_path, 'model_AL00_multiVar_LKJ.stan')
stanmodel_AL00_multiVar_LKJ <- cmdstan_model(stan_file_AL00_multiVar_LKJ)
stanmodel_AL00_multiVar_LKJ$exe_file()

fit_AL00_multiVar_LKJ_indiv_0820 = stanmodel_AL00_multiVar_LKJ$sample(
  data = ReinforcementLearningStanData_indiv_0820, seed = 777,
  init = function(chain_id) {
    list(
        mu_beta = runif(1, -1, 1),
        mu_alpha = runif(1, -2, 2)
        )
  }
  # # == VB method (an unreliable, but really fast estimation method)
  # algorithm = "meanfield", #'meanfield' "fullrank"
  # iter = 1000 #(Default)
  # )
  # # == END - VB method ==

  # MCMC sampling
  , chains = chains
  , parallel_chains = parallel_chains
  , thin = thin
  , iter_warmup = iter_warmup
  , iter_sampling = iter_sampling
)


```

### Group condition
```{r eval = FALSE}
stan_file_SL00_multiVar_LKJ <- file.path(this_path, 'model_SL00_multiVar_LKJ.stan')
stanmodel_SL00_multiVar_LKJ <- cmdstan_model(stan_file_SL00_multiVar_LKJ)
stanmodel_SL00_multiVar_LKJ$exe_file()

# data for stan fitting - social learning
completedIDs = which(table(allBehaviour0820$amazonID) >= 36) %>% names()
allBehaviour0820_social = allBehaviour0820 %>% dplyr::filter(amazonID %in% completedIDs) %>%
  dplyr::filter(indivOrGroup == 1) # note this is only the social condition
allBehaviour0820_social$sub = as.numeric(as.factor(allBehaviour0820_social$amazonID))
allBehaviour0820_social$group = as.numeric(as.factor(allBehaviour0820_social$room))

# insert missed trials
for(i in 1:length(table(allBehaviour0820_social$amazonID))) {
    thisSubject <- allBehaviour0820_social %>% dplyr::filter(sub==i)
    for(t in 1:70) {
        if(nrow(thisSubject[which(thisSubject$round==t),])==0) {
            newRow = rep(NA, ncol(allBehaviour0820_social))
            names(newRow) <- names(allBehaviour0820_social)
            allBehaviour0820_social <- bind_rows(allBehaviour0820_social, newRow)
            allBehaviour0820_social$choice_num[nrow(allBehaviour0820_social)] = -1
            allBehaviour0820_social$payoff[nrow(allBehaviour0820_social)] = -1
            allBehaviour0820_social$round[nrow(allBehaviour0820_social)] = t
            allBehaviour0820_social$socialFreq_safe[nrow(allBehaviour0820_social)] = 0
            allBehaviour0820_social$socialFreq_risky[nrow(allBehaviour0820_social)] = 0
            allBehaviour0820_social$amazonID[nrow(allBehaviour0820_social)] = allBehaviour0820_social$amazonID[which(allBehaviour0820_social$sub==i)][1]
            allBehaviour0820_social$sub[nrow(allBehaviour0820_social)] = allBehaviour0820_social$sub[which(allBehaviour0820_social$sub==i)][1]
            allBehaviour0820_social$group[nrow(allBehaviour0820_social)] = allBehaviour0820_social$group[which(allBehaviour0820_social$sub==i)][1]
            #allBehaviour0820_social$taskDifficulty_num[nrow(allBehaviour0820_social)] = allBehaviour0820_social$taskDifficulty_num[which(allBehaviour0820_social$sub==i)][1]
        }
    }
}
allBehaviour0820_social = allBehaviour0820_social[order(allBehaviour0820_social$round),] # Sorting by round
allBehaviour0820_social = allBehaviour0820_social %>% group_by(amazonID) %>% arrange(round, .by_group = TRUE)

# preparing data fed to Stan
ReinforcementLearningStanData_group_0820 = list(
    All = nrow(allBehaviour0820_social),
    Nsub = length(table(allBehaviour0820_social$amazonID)),
    Ncue = 2, # number of options
    Ntrial = 70,
    Ncon = 1,
    sub = allBehaviour0820_social$sub,
    Y = allBehaviour0820_social$choice_num + 1,
    trial = allBehaviour0820_social$round,
    payoff = allBehaviour0820_social$payoff / 100
    )

## ==========  Data reading and cleaning
## compfun() speeds up the calculation
library(compiler)
F_calculation = function(array, Nsub, Ntrial, data)
{
    F <- array
    # F (Note: F[sub, m, t] means the frequency informatin made at 't-1')
    for(i in 1:Nsub) {
        F[i,1,1] = 0; F[i,2,1] = 0; # F[i,3,1] = 0;
        for(t in 2:Ntrial) {
            lastChoice = 0
            if( subset(data,sub==i&round==(t-1))$choice_num+1>0 ) {
                lastChoice = subset(data,sub==i&round==(t-1))$choice_num + 1
            }
            F[i,1,t] <- data %>% dplyr::filter(sub==i&round==t) %>% .$socialFreq_safe
            F[i,2,t] <- data %>% dplyr::filter(sub==i&round==t) %>% .$socialFreq_risky
            #F[i,3,t] = subset(data,sub==i&round==t)$socialFreq2
            if(lastChoice>0){
                F[i,lastChoice,t] = F[i,lastChoice,t] - 1
            }
            if(length(which(F[i,,t]>0))==0){
                F[i,,t] <- c(-1,-1)
            }
        }
    }
    return(F)
}

F_calculation.compiled <- cmpfun(F_calculation)
F0 = array(rep(NA,nrow(allBehaviour0820_social)), c(ReinforcementLearningStanData_group_0820$Nsub, ReinforcementLearningStanData_group_0820$Ncue, ReinforcementLearningStanData_group_0820$Ntrial))
F = F_calculation.compiled(F0, ReinforcementLearningStanData_group_0820$Nsub, ReinforcementLearningStanData_group_0820$Ntrial,allBehaviour0820_social)

ReinforcementLearningStanData_group_0820$F = F
## ==========  Data cleaning END

## =======================================================================
## SL00_multiVar_LKJ model fitting
#  =======================================================================

fit_SL00_multiVar_LKJ_0820 = stanmodel_SL00_multiVar_LKJ$sample(
  data = ReinforcementLearningStanData_group_0820,
  seed = 777, 
  init = function(chain_id) {
    list(
        mu_beta = runif(1, -1, 1),
        mu_alpha = runif(1, -3, 3),
        mu_soc0 = runif(1, -2, 2),
        mu_theta = runif(1, -2, 2)
        )
  },
  # # == VB method! ==
  # algorithm = "meanfield", #'meanfield' "fullrank"
  # iter = 1000 #(Default)
  # )
  # # == END - VB method! ==

  chains = chains, parallel_chains = parallel_chains, thin = thin, iter_warmup = iter_warmup, iter_sampling = iter_sampling
)

```


## Results of the model fitting
The estimated parameter values are loaded in the following chunk. 
```{r}
fit_AL00_multiVar_LKJ_indiv_0820_globalparameters <- read.csv('fit_AL00_multiVar_LKJ_indiv_0820_globalparameters.csv')
fit_AL00_multiVar_LKJ_indiv_0820_parameters <- read.csv('fit_AL00_multiVar_LKJ_indiv_0820_parameters.csv')
fit_SL00_multiVar_LKJ_0820_globalparameters <- read.csv('fit_SL00_multiVar_LKJ_0820_globalparameters.csv')
fit_SL00_multiVar_LKJ_0820_parameters <- read.csv('fit_SL00_multiVar_LKJ_0820_parameters.csv')
```

The followings are the fit of the global parameters (corresponding to the values shown in Table 1 of the main text). 

```{r}
# Global parameters for the individual condition (individual learning model)
fit_AL00_multiVar_LKJ_indiv_0820_globalparameters[1:4,] %>% kable(format = "markdown")
# Global parameters for the group condition (social learning model)
fit_SL00_multiVar_LKJ_0820_globalparameters[1:8,] %>% kable(format = "markdown")
```

# The 1-risky-3-safe task

## The task
The optimal risky option produces either 50 or 425 points at probability 0.6 and 0.4, respectively (the expected payoff was 200). The 3 safe options each dropped 150, 125, and 100 points respectively, with a small Gaussian noise with s.d. = 5. 

## Individual condition
```{r}
allBehaviour1022_indiv <- read.csv("allBehaviour1022_indiv.csv")
allBehaviour1022_indiv_riskID11 <- allBehaviour1022_indiv %>% 
  filter(riskDistributionId_factor=='Con: 0') 
# The 1-risky-3-safe task was labeled "Con: 0" or "11" originally
# And in the analysis code, things like "riskID11" means the 1-risky-3-safe task
# while "riskID12" means the 2-risky-2-safe task
allBehaviour1022_indiv_riskID11$sub_old <- allBehaviour1022_indiv_riskID11$sub
allBehaviour1022_indiv_riskID11$sub <- allBehaviour1022_indiv_riskID11$amazonID %>% as.factor() %>% as.numeric()

# preparing data fed to Stan
ReinforcementLearningStanData_indiv_riskID11 = list(
    All = nrow(allBehaviour1022_indiv_riskID11),
    Nsub = length(table(allBehaviour1022_indiv_riskID11$amazonID)),
    Ncue = 4, # number of options
    Ntrial = 70,
    Ncon = 1,
    sub = allBehaviour1022_indiv_riskID11$sub,
    Y = allBehaviour1022_indiv_riskID11$choice_num + 1,
    trial = allBehaviour1022_indiv_riskID11$round,
    payoff = allBehaviour1022_indiv_riskID11$payoff / 100
    )
# number of subject in the individual condition
ReinforcementLearningStanData_indiv_riskID11$Nsub
```

## Model fitting (skippable)
```{r eval=FALSE}
# individual condition
fit_AL00_multiVar_LKJ_indiv_riskID11 = stanmodel_AL00_multiVar_LKJ$sample(
  data = ReinforcementLearningStanData_indiv_riskID11, seed = 777,
  init = function(chain_id) {
    list(
        mu_beta = runif(1, -1, 1),
        mu_alpha = runif(1, -2, 2)
        )
  },
  # # == VB method! ==
  # algorithm = "meanfield", #'meanfield' "fullrank"
  # iter = 1000 #(Default)
  # )
  # # == END - VB method! ==

  # MCMC sampling
  chains = chains, parallel_chains = parallel_chains, thin = thin, iter_warmup = iter_warmup, iter_sampling = iter_sampling
)
fit_AL00_multiVar_LKJ_indiv_riskID11_indiv_riskID11Condition_globalparameters = fit_AL00_multiVar_LKJ_indiv_riskID11$summary(c('mu_alpha','mu_beta','s_alpha','s_beta','Rho_ID'))
```

## Estimation of the individual learning
```{r}
fit_AL00_multiVar_LKJ_indiv_riskID11_indiv_riskID11Condition_globalparameters <- read.csv('fit_AL00_multiVar_LKJ_indiv_riskID11_indiv_riskID11Condition_globalparameters.csv')
fit_AL00_multiVar_LKJ_indiv_riskID11_indiv_riskID11Condition_globalparameters[1:4,] %>% kable(format = "markdown")
```


# The 2-risky-2-safe task

## The task
The optimal risky option produces either 50 or 425 points at probability 0.6 and 0.4, respectively (the expected payoff = 200). The two safe options each dropped 150 and 125 points, respectively, with a small Gaussian noise with s.d. = 5. The suboptimal risky option whose expected value was 125, produced either 50 or 238 points at probability 0.6 and 0.4, respectively. 

## Individual condition
```{r}
allBehaviour1022_indiv <- read.csv("allBehaviour1022_indiv.csv")
allBehaviour1022_indiv_riskID12 <- allBehaviour1022_indiv %>% filter(riskDistributionId_factor=='Con: 2') #'Con: 2' means the 2-risky-2-safe task
allBehaviour1022_indiv_riskID12$sub_old <- allBehaviour1022_indiv_riskID12$sub
allBehaviour1022_indiv_riskID12$sub <- allBehaviour1022_indiv_riskID12$amazonID %>% as.factor() %>% as.numeric()

# preparing data fed to Stan
ReinforcementLearningStanData_indiv_riskID12 = list(
    All = nrow(allBehaviour1022_indiv_riskID12),
    Nsub = length(table(allBehaviour1022_indiv_riskID12$amazonID)),
    Ncue = 4, # number of options
    Ntrial = 70,
    Ncon = 1,
    sub = allBehaviour1022_indiv_riskID12$sub,
    Y = allBehaviour1022_indiv_riskID12$choice_num + 1,
    trial = allBehaviour1022_indiv_riskID12$round,
    payoff = allBehaviour1022_indiv_riskID12$payoff / 100
    )

# number of subject in the individual condition
ReinforcementLearningStanData_indiv_riskID12$Nsub
```

## Model fitting (skippable)
```{r eval=FALSE}
fit_AL00_multiVar_LKJ_indiv_riskID12 = stanmodel_AL00_multiVar_LKJ$sample(
  data = ReinforcementLearningStanData_indiv_riskID12, seed = 777,
  init = function(chain_id) {
    list(
        mu_beta = runif(1, -1, 1),
        mu_alpha = runif(1, -2, 2)
        )
  },
  # # == VB method! ==
  # algorithm = "meanfield", #'meanfield' "fullrank"
  # iter = 1000 #(Default)
  # )
  # # == END - VB method! ==

  # MCMC sampling
  chains = chains, parallel_chains = parallel_chains, thin = thin, iter_warmup = iter_warmup, iter_sampling = iter_sampling
)
fit_AL00_multiVar_LKJ_indiv_riskID12_indiv_riskID12Condition_globalparameters = fit_AL00_multiVar_LKJ_indiv_riskID12$summary(c('mu_alpha','mu_beta','s_alpha','s_beta','Rho_ID'))
```

## Estimation of the individual learning
```{r}
fit_AL00_multiVar_LKJ_indiv_riskID12_indiv_riskID12Condition_globalparameters <- read.csv('fit_AL00_multiVar_LKJ_indiv_riskID12_indiv_riskID12Condition_globalparameters.csv')
fit_AL00_multiVar_LKJ_indiv_riskID12_indiv_riskID12Condition_globalparameters[1:4,] %>% kable(format = "markdown")
```

# Group conditions for both 4-armed bandit task

Rather than running a MCMC separately for each of the two different tasks, I ran a single MCMC for the group conditions of these two variations 4-armed bandit tasks. The assumptions of the model were basically unchanged compared to the previous fitting analyses shown above; however, the assumption for the covariances between the global parameters were modefied. Here, a single set of covariances are shared in both tasks. That is, although the global parameters were still estimated for each of the two tasks, the potential correlation structures between parameter-values were assumed to be the same for both tasks. This practice dramatically improved the convergence. We confirmed that the estimated parameter values were not hugely changed compared to the standard fitting practice.

## Data 
```{r}
# behaivoural data
allBehaviour1022_group <- read.csv("allBehaviour1022_group.csv")
allBehaviour1022_group$sub <- allBehaviour1022_group$amazonID %>% as.factor() %>% as.numeric()
allBehaviour1022_group$task <- 1
allBehaviour1022_group$task[which(allBehaviour1022_group$riskDistributionId==12)] <- 2

summary_each_individual <- allBehaviour1022_group %>%
	group_by(sub) %>%
	summarise(mean_risky_choice_rate = mean(risky_choice), na.rm=TRUE
		, mean_payoff = mean(payoff/100)
		, task = task[1]
		)

# preparing data fed to Stan
ReinforcementLearningStanData_group = list(
    All = nrow(allBehaviour1022_group),
    Nsub = length(table(allBehaviour1022_group$amazonID)),
    Ncue = 4, # number of options
    Ntrial = 70,
    Ncon = 2, # number of conditions, which is two because I considered two tasks here
    sub = allBehaviour1022_group$sub,
    Y = allBehaviour1022_group$choice_num + 1,
    trial = allBehaviour1022_group$round,
    payoff = allBehaviour1022_group$payoff / 100,
    condition = summary_each_individual$task
    )
```


```{r eval=FALSE}
# Calculating F
F_calculation_4ab = function(array, Nsub, Ntrial, data)
{
    F <- array
    # F (Simulation data とは t+1 ずれてるから注意！)
    for(i in 1:Nsub) {
        F[i,1,1] = 0; F[i,2,1] = 0; F[i,3,1] = 0; F[i,4,1] = 0;
        for(t in 2:Ntrial) {
            lastChoice = 0
            if( subset(data,sub==i&round==(t-1))$choice_num+1>0 ) {
                lastChoice = subset(data,sub==i&round==(t-1))$choice_num + 1
            }
            F[i,1,t] <- data %>% dplyr::filter(sub==i&round==t) %>% .$socialFreq_safe1
            F[i,2,t] <- data %>% dplyr::filter(sub==i&round==t) %>% .$socialFreq_safe2
            F[i,3,t] <- data %>% dplyr::filter(sub==i&round==t) %>% .$socialFreq_safe3
            F[i,4,t] <- data %>% dplyr::filter(sub==i&round==t) %>% .$socialFreq_risky
            # F[i,3,t] = subset(data,sub==i&round==t)$socialFreq2
            if(lastChoice>0){
                F[i,lastChoice,t] = F[i,lastChoice,t] - 1
            }
            if(length(which(F[i,,t]>0))==0){
                F[i,,t] <- c(-1,-1,-1,-1)
            }
        }
    }
    return(F)
}

F_calculation_4ab.compiled <- cmpfun(F_calculation_4ab)
F0 = array(rep(NA,nrow(allBehaviour1022_group)), c(ReinforcementLearningStanData_group$Nsub, ReinforcementLearningStanData_group$Ncue, ReinforcementLearningStanData_group$Ntrial))
F = F_calculation_4ab.compiled(F0, ReinforcementLearningStanData_group$Nsub, ReinforcementLearningStanData_group$Ntrial, allBehaviour1022_group)

ReinforcementLearningStanData_group$F = F
## ==========  Data cleaning END
```

## Model fitting (skippable)
```{r eval=FALSE}
# stan model
stan_file_SL00_multiVar_LKJ <- file.path('model_SL00_multiVar_LKJ_multi.stan')
stanmodel_SL00_multiVar_LKJ <- cmdstan_model(stan_file_SL00_multiVar_LKJ)
stanmodel_SL00_multiVar_LKJ$exe_file()
num_parameter <- 4

fit_SL00_multiVar_LKJ_1022 = stanmodel_SL00_multiVar_LKJ$sample(
  data = ReinforcementLearningStanData_group
  , seed = 777 
  , init = function(chain_id) {
    list(mu_beta = runif(ReinforcementLearningStanData_group$Ncon, -2, 2)
        , mu_alpha = runif(ReinforcementLearningStanData_group$Ncon, -2, 2)
        , mu_soc0 = runif(ReinforcementLearningStanData_group$Ncon, -3, 1)
        , mu_theta = runif(ReinforcementLearningStanData_group$Ncon, -1, 3)
        )
  }
  , adapt_delta = 0.9 # default 0.8

  , chains = chains, parallel_chains = parallel_chains, thin = thin, iter_warmup = iter_warmup, iter_sampling = iter_sampling 
)

pars <- c('mu_alpha'
    ,'mu_beta'
    ,'mu_soc0'
    ,'mu_theta'
    ,'s_alpha'
    ,'s_beta'
    ,'s_soc0'
    ,'s_theta'
    )

# save global parameters
fit_SL00_multiVar_LKJ_1022_globalparameters = fit_SL00_multiVar_LKJ_1022$summary(c(pars,'Rho_ID'))
```

## Results of the fitting

Note that each global parameter value contains two values. The value with `[1]` means the 1-risky-3-safe task, whereas the `[2]` means the 2-risky-2-safe task. The values are corresponding to Table 1 in the main text. 

```{r}
fit_SL00_multiVar_LKJ_1022_globalparameters <- read.csv('fit_SL00_multiVar_LKJ_1022_globalparameters.csv')
fit_SL00_multiVar_LKJ_1022_globalparameters[1:16,] %>% kable(format = "markdown")
```

# A post-hoc simulation

To check if the fit model could nicely recover the behavioural pattern observed, I ran a post-hoc simulation in a separated R script (see `posthoc_simulation.R`). The simulation produced the following data:

```{r}
social_learning_model_validation_0820_data <- read.csv('social_learning_model_validation_0820_data.csv')
social_learning_model_validation_1022_riskID11_data <- read.csv('social_learning_model_validation_1022_riskID11_data.csv')
social_learning_model_validation_1022_riskID12_data <- read.csv('social_learning_model_validation_1022_riskID12_data.csv')
```

```{r include=FALSE}
# ==== The aim of this chunk =====
# Though the following messy code, this chink will match the participant's 
# performance with their fit "susceptibility" (i.e. alpha * (beta + 1)). 
# In theory, the susceptibility should be negatively related to 
# the probability of risky choice.
#

# ======================================
# 1-risky 1-safe (2-armed) task
# ======================================
fit_AL00_multiVar_LKJ_indiv_0820_globalparameters <- read.csv('fit_AL00_multiVar_LKJ_indiv_0820_globalparameters.csv')
fit_AL00_multiVar_LKJ_indiv_0820_parameters <- read.csv('fit_AL00_multiVar_LKJ_indiv_0820_parameters.csv')
fit_SL00_multiVar_LKJ_0820_globalparameters <- read.csv('fit_SL00_multiVar_LKJ_0820_globalparameters.csv')
fit_SL00_multiVar_LKJ_0820_parameters <- read.csv('fit_SL00_multiVar_LKJ_0820_parameters.csv')

behaviour_main_0820 <- read.csv("behaviour_main_0820.csv")
behaviour_indiv_0820 <-read.csv("behaviour_indiv_0820.csv")
allBehaviour0820 <- rbind(behaviour_main_0820, behaviour_indiv_0820)
allBehaviour0820 <- allBehaviour0820 %>% 
  dplyr::filter(amazonID != 'INHOUSETEST2') %>%  # eliminating data generated by debug tests
  dplyr::filter(amazonID != '5eac70db94edd22d57fa00c4') # a bug in the data storaging process

# make the choice data binary
allBehaviour0820$choice_num = NA
allBehaviour0820$choice_num[which(allBehaviour0820$choice=='sure')] = 0
allBehaviour0820$choice_num[which(allBehaviour0820$choice=='risky')] = 1
allBehaviour0820$choice_num[which(allBehaviour0820$choice=='miss')] = -1

allBehaviour0820$riskDistributionId_factor = 'Condition 5'
allBehaviour0820$riskDistributionId_factor[which(allBehaviour0820$riskDistributionId==6)] = 'Condition 6'
allBehaviour0820$riskDistributionId_factor[which(allBehaviour0820$riskDistributionId==7)] = 'Condition 7'

allBehaviour0820$indivOrGroup_factor = 'Individual'
allBehaviour0820$indivOrGroup_factor[allBehaviour0820$indivOrGroup == 1] = 'Social'

allBehaviour0820$groupSize_category = 'Small'
allBehaviour0820$groupSize_category[which(allBehaviour0820$groupSize==1)] = 'Individual'
allBehaviour0820$groupSize_category[which(allBehaviour0820$groupSize>4)] = 'Large'
allBehaviour0820$groupSize_category = factor(allBehaviour0820$groupSize_category, levels = c('Individual','Small','Large'))

# individual condition
completedIDs = which(table(allBehaviour0820$amazonID) >= 36) %>% names()
allBehaviour0820_indiv = allBehaviour0820 %>% dplyr::filter(amazonID %in% completedIDs) %>%
  dplyr::filter(indivOrGroup == 0) # note this is only the individual condition
allBehaviour0820_indiv$sub = as.numeric(as.factor(allBehaviour0820_indiv$amazonID))
allBehaviour0820_indiv = allBehaviour0820_indiv %>% group_by(amazonID) %>% arrange(round, .by_group = TRUE)

# summarised data
allBehaviour0820_indiv_summarised_t35 = allBehaviour0820_indiv %>%
	dplyr::filter(round>35) %>%
	group_by(sub) %>%
	summarise(
		risky_choice_count = sum(choice_num, na.rm = TRUE),
		risky_choice_mean = mean(choice_num, na.rm=TRUE),
		trial_num = n(),
		indivOrGroup_factor = indivOrGroup_factor[1],
		room = room[1],
		amazonID = amazonID[1]
	)
allBehaviour0820_indiv_summarised_t35$groupID = allBehaviour0820_indiv_summarised_t35$room
allBehaviour0820_indiv_summarised_t35$groupID[which(allBehaviour0820_indiv_summarised_t35$indivOrGroup_factor=='Individual')] = 'Individual'

# Individual Condition Only
parameterfit_indiv_AL00_0820 <- left_join(fit_AL00_multiVar_LKJ_indiv_0820_parameters, allBehaviour0820_indiv_summarised_t35, by = 'sub')
parameterfit_indiv_AL00_0820$hot_stove_susceptibility <- parameterfit_indiv_AL00_0820$alpha_median_AL00_multiVar_LKJ * (parameterfit_indiv_AL00_0820$beta_median_AL00_multiVar_LKJ + 1)
# if the hot stove effect is too large
parameterfit_indiv_AL00_0820$hot_stove_susceptibility_trancated <- parameterfit_indiv_AL00_0820$hot_stove_susceptibility
parameterfit_indiv_AL00_0820$hot_stove_susceptibility_trancated[which(parameterfit_indiv_AL00_0820$hot_stove_susceptibility > 6)] <- 6


# Group condition
completedIDs = which(table(allBehaviour0820$amazonID) >= 36) %>% names()
allBehaviour0820_social = allBehaviour0820 %>% dplyr::filter(amazonID %in% completedIDs) %>%
  dplyr::filter(indivOrGroup == 1) # note this is only the social condition
allBehaviour0820_social$sub = as.numeric(as.factor(allBehaviour0820_social$amazonID))
allBehaviour0820_social$group = as.numeric(as.factor(allBehaviour0820_social$room))
# summarised data
allBehaviour0820_social_summarised_t35 = allBehaviour0820_social %>%
	dplyr::filter(round>35) %>%
	group_by(sub) %>%
	summarise(
		risky_choice_count = sum(choice_num, na.rm = TRUE),
		risky_choice_mean = mean(choice_num, na.rm=TRUE),
		trial_num = n(),
		indivOrGroup_factor = indivOrGroup_factor[1],
		room = room[1],
		amazonID = amazonID[1]
	)
allBehaviour0820_social_summarised_t35$groupID = allBehaviour0820_social_summarised_t35$room
allBehaviour0820_social_summarised_t35$groupID[which(allBehaviour0820_social_summarised_t35$indivOrGroup_factor=='Individual')] = 'Individual'

# Group condition
fit_parameters_group_SL00_mcmc <- left_join(fit_SL00_multiVar_LKJ_0820_parameters, allBehaviour0820_social_summarised_t35, by = 'sub')
fit_parameters_group_SL00_mcmc$hot_stove_susceptibility <- fit_parameters_group_SL00_mcmc$alpha_median_SL00_multiVar_LKJ * (fit_parameters_group_SL00_mcmc$beta_median_SL00_multiVar_LKJ + 1)
# if the hot stove effect is too large
fit_parameters_group_SL00_mcmc$hot_stove_susceptibility_trancated <- fit_parameters_group_SL00_mcmc$hot_stove_susceptibility
fit_parameters_group_SL00_mcmc$hot_stove_susceptibility_trancated[which(fit_parameters_group_SL00_mcmc$hot_stove_susceptibility > 6)] <- 6

# overall means
social_learning_model_validation_0820_summary <-
	social_learning_model_validation_0820_data %>%
	group_by(condition_dummy, hot_stove_susceptibility_rounded, soc_mean_category) %>%
	summarise(
		proportionRiskyChoice_b2_mean = mean(proportionRiskyChoice_b2),
		proportionRiskyChoice_b2_sd = sd(proportionRiskyChoice_b2),
		raw_proportionRiskyChoice_b2_mean = proportionRiskyChoice_b2 %>% convert_alpha_to_alphaRaw() %>% mean(),
		raw_proportionRiskyChoice_b2_sd = proportionRiskyChoice_b2 %>% convert_alpha_to_alphaRaw() %>% sd(),
		soc_mean = mean(soc_mean),
		n = n()
		)

social_learning_model_validation_0820_summary$proportionRiskyChoice_b2_lower <-
	(social_learning_model_validation_0820_summary$raw_proportionRiskyChoice_b2_mean - social_learning_model_validation_0820_summary$raw_proportionRiskyChoice_b2_sd / sqrt(social_learning_model_validation_0820_summary$n)) %>% convert_alphaRaw_to_alpha
social_learning_model_validation_0820_summary$proportionRiskyChoice_b2_upper <-
	(social_learning_model_validation_0820_summary$raw_proportionRiskyChoice_b2_mean + social_learning_model_validation_0820_summary$raw_proportionRiskyChoice_b2_sd / sqrt(social_learning_model_validation_0820_summary$n)) %>% convert_alphaRaw_to_alpha

social_learning_model_validation_0820_summary$proportionRiskyChoice_b2_mid <-
	social_learning_model_validation_0820_summary$raw_proportionRiskyChoice_b2_mean %>% convert_alphaRaw_to_alpha


# modest social learners' means
social_learning_model_validation_0820_summary_reallyHighSigma <-
	social_learning_model_validation_0820_data %>%
	dplyr::filter(soc_mean > 5/10 & soc_mean < 8/10 & hot_stove_susceptibility_rounded < 6) %>%
	group_by(condition_dummy, hot_stove_susceptibility_rounded, soc_mean_category) %>%
	summarise(
		proportionRiskyChoice_b2_mean = mean(proportionRiskyChoice_b2),
		proportionRiskyChoice_b2_sd = sd(proportionRiskyChoice_b2),
		raw_proportionRiskyChoice_b2_mean = proportionRiskyChoice_b2 %>% convert_alpha_to_alphaRaw() %>% median(),
		raw_proportionRiskyChoice_b2_sd = proportionRiskyChoice_b2 %>% convert_alpha_to_alphaRaw() %>% sd(),
		soc_mean = mean(soc_mean),
		n = n()
		)

social_learning_model_validation_0820_summary_reallyHighSigma$proportionRiskyChoice_b2_mid <-
	social_learning_model_validation_0820_summary_reallyHighSigma$raw_proportionRiskyChoice_b2_mean %>% convert_alphaRaw_to_alpha

fit_parameters_group_SL00_mcmc$soc_mean <- fit_parameters_group_SL00_mcmc$soc_mean_SL00_multiVar_LKJ

# ======================================
# 1-risky 3-safe (4-armed) task
# ======================================

# fit result -- global parameters
fit_SL00_multiVar_LKJ_1022_globalparameters <- read.csv('fit_SL00_multiVar_LKJ_1022_globalparameters.csv')
fit_AL00_multiVar_LKJ_indiv_riskID11_indiv_riskID11Condition_globalparameters <- read.csv('fit_AL00_multiVar_LKJ_indiv_riskID11_indiv_riskID11Condition_globalparameters.csv')

## behavioural data summary
allBehaviour1022_group <- read.csv("allBehaviour1022_group.csv")
allBehaviour1022_group_riskID11 <- allBehaviour1022_group %>% dplyr::filter(riskDistributionId == 11)

fit_SL00_multiVar_LKJ_1022_parameters <- read.csv("fit_SL00_multiVar_LKJ_1022_parameters.csv")

allBehaviour1022_group_riskID11_summarised_t35 <- allBehaviour1022_group_riskID11 %>%
	dplyr::filter(round>35) %>%
	group_by(amazonID, sub) %>%
	summarise(
		risky_choice_count = sum(best_risky_choice, na.rm = TRUE),
		risky_choice_mean = mean(best_risky_choice, na.rm=TRUE),
		trial_num = n(),
		indivOrGroup_factor = indivOrGroup_factor[1],
		room = room[1]
	)

allBehaviour1022_group_riskID11_summarised_t35$groupID <- allBehaviour1022_group_riskID11_summarised_t35$room


allBehaviour1022_indiv <- read.csv("allBehaviour1022_indiv.csv")
allBehaviour1022_indiv_riskID11 <- allBehaviour1022_indiv %>% 
  filter(riskDistributionId_factor=='Con: 0') 
# The 1-risky-3-safe task was labeled "Con: 0" or "11" originally
# And in the analysis code, things like "riskID11" means the 1-risky-3-safe task
# while "riskID12" means the 2-risky-2-safe task
allBehaviour1022_indiv_riskID11$sub_old <- allBehaviour1022_indiv_riskID11$sub
allBehaviour1022_indiv_riskID11$sub <- allBehaviour1022_indiv_riskID11$amazonID %>% as.factor() %>% as.numeric()

allBehaviour1022_indiv_riskID11_summarised_t35 <- allBehaviour1022_indiv_riskID11 %>%
	dplyr::filter(round>35) %>%
	group_by(amazonID, sub) %>%
	summarise(
		risky_choice_count = sum(best_risky_choice, na.rm = TRUE),
		risky_choice_mean = mean(best_risky_choice, na.rm=TRUE),
		trial_num = n(),
		indivOrGroup_factor = indivOrGroup_factor[1],
		room = room[1]
	)
allBehaviour1022_indiv_riskID11_summarised_t35$groupID = allBehaviour1022_indiv_riskID11_summarised_t35$room
allBehaviour1022_indiv_riskID11_summarised_t35$groupID[which(allBehaviour1022_indiv_riskID11_summarised_t35$indivOrGroup_factor=='Individual')] = 'Individual'

# Individual fits
fit_AL00_multiVar_LKJ_indiv_riskID11_parameters <- read.csv('fit_AL00_multiVar_LKJ_indiv_riskID11_parameters.csv')

# Merging the behavioural data with the fit parameters
# hot stove effect - individual
fit_AL_indiv_riskID11_parameters <- right_join(fit_AL00_multiVar_LKJ_indiv_riskID11_parameters, allBehaviour1022_indiv_riskID11_summarised_t35, by = 'sub')
fit_AL_indiv_riskID11_parameters$hot_stove_susceptibility <- fit_AL_indiv_riskID11_parameters$alpha_median_AL00_multiVar_LKJ * (1+ fit_AL_indiv_riskID11_parameters$beta_median_AL00_multiVar_LKJ)
fit_AL_indiv_riskID11_parameters$hot_stove_susceptibility_trancated <- fit_AL_indiv_riskID11_parameters$hot_stove_susceptibility
fit_AL_indiv_riskID11_parameters$hot_stove_susceptibility_trancated[which(fit_AL_indiv_riskID11_parameters$hot_stove_susceptibility > 6)] <- 6

# hot stove effect - group
fit_SL00_riskID11_parameters <- right_join(fit_SL00_multiVar_LKJ_1022_parameters, allBehaviour1022_group_riskID11_summarised_t35, by = 'sub')
fit_SL00_riskID11_parameters$hot_stove_susceptibility <- fit_SL00_riskID11_parameters$alpha_mean_SL00_multiVar_LKJ * (1+ fit_SL00_riskID11_parameters$beta_mean_SL00_multiVar_LKJ)
fit_SL00_riskID11_parameters$hot_stove_susceptibility_trancated <- fit_SL00_riskID11_parameters$hot_stove_susceptibility
fit_SL00_riskID11_parameters$hot_stove_susceptibility_trancated[which(fit_SL00_riskID11_parameters$hot_stove_susceptibility > 6)] <- 6

# overall means
social_learning_model_validation_1022_riskID11_summary <-
	social_learning_model_validation_1022_riskID11_data %>%
	group_by(condition_dummy, hot_stove_susceptibility_rounded, soc_mean_category) %>%
	summarise(
		proportionRiskyChoice_b2_mean = mean(proportionRiskyChoice_b2),
		proportionRiskyChoice_b2_sd = sd(proportionRiskyChoice_b2),
		raw_proportionRiskyChoice_b2_mean = proportionRiskyChoice_b2 %>% convert_alpha_to_alphaRaw() %>% mean(),
		raw_proportionRiskyChoice_b2_sd = proportionRiskyChoice_b2 %>% convert_alpha_to_alphaRaw() %>% sd(),
		soc_mean = mean(soc_mean),
		n = n()
		)

social_learning_model_validation_1022_riskID11_summary$proportionRiskyChoice_b2_lower <-
	(social_learning_model_validation_1022_riskID11_summary$raw_proportionRiskyChoice_b2_mean - social_learning_model_validation_1022_riskID11_summary$raw_proportionRiskyChoice_b2_sd / sqrt(social_learning_model_validation_1022_riskID11_summary$n)) %>% convert_alphaRaw_to_alpha
social_learning_model_validation_1022_riskID11_summary$proportionRiskyChoice_b2_upper <-
	(social_learning_model_validation_1022_riskID11_summary$raw_proportionRiskyChoice_b2_mean + social_learning_model_validation_1022_riskID11_summary$raw_proportionRiskyChoice_b2_sd / sqrt(social_learning_model_validation_1022_riskID11_summary$n)) %>% convert_alphaRaw_to_alpha

social_learning_model_validation_1022_riskID11_summary$proportionRiskyChoice_b2_mid <-
	social_learning_model_validation_1022_riskID11_summary$raw_proportionRiskyChoice_b2_mean %>% convert_alphaRaw_to_alpha


# modest social learners' means
social_learning_model_validation_1022_riskID11_summary_reallyHighSigma <-
	social_learning_model_validation_1022_riskID11_data %>%
	dplyr::filter(soc_mean > 5/10 & soc_mean < 8/10 & hot_stove_susceptibility_rounded < 6) %>%
	group_by(condition_dummy, hot_stove_susceptibility_rounded, soc_mean_category) %>%
	summarise(
		proportionRiskyChoice_b2_mean = mean(proportionRiskyChoice_b2),
		proportionRiskyChoice_b2_sd = sd(proportionRiskyChoice_b2),
		raw_proportionRiskyChoice_b2_mean = proportionRiskyChoice_b2 %>% convert_alpha_to_alphaRaw() %>% median(),
		raw_proportionRiskyChoice_b2_sd = proportionRiskyChoice_b2 %>% convert_alpha_to_alphaRaw() %>% sd(),
		soc_mean = mean(soc_mean),
		n = n()
		)

social_learning_model_validation_1022_riskID11_summary_reallyHighSigma$proportionRiskyChoice_b2_mid <-
	social_learning_model_validation_1022_riskID11_summary_reallyHighSigma$raw_proportionRiskyChoice_b2_mean %>% convert_alphaRaw_to_alpha

# ======================================
# 2-risky 2-safe (4-armed) task
# ======================================

# fit result -- global parameters
fit_SL00_multiVar_LKJ_1022_globalparameters <- read.csv('fit_SL00_multiVar_LKJ_1022_globalparameters.csv')
fit_AL00_multiVar_LKJ_indiv_riskID12_indiv_riskID12Condition_globalparameters <- read.csv('fit_AL00_multiVar_LKJ_indiv_riskID12_indiv_riskID12Condition_globalparameters.csv')

## behavioural data summary
allBehaviour1022_group <- read.csv("allBehaviour1022_group.csv")
allBehaviour1022_group_riskID12 <- allBehaviour1022_group %>% dplyr::filter(riskDistributionId == 12)

fit_SL00_multiVar_LKJ_1022_parameters <- read.csv("fit_SL00_multiVar_LKJ_1022_parameters.csv")

allBehaviour1022_group_riskID12_summarised_t35 <- allBehaviour1022_group_riskID12 %>%
	dplyr::filter(round>35) %>%
	group_by(amazonID, sub) %>%
	summarise(
		risky_choice_count = sum(best_risky_choice, na.rm = TRUE),
		risky_choice_mean = mean(best_risky_choice, na.rm=TRUE),
		trial_num = n(),
		indivOrGroup_factor = indivOrGroup_factor[1],
		room = room[1]
	)

allBehaviour1022_group_riskID12_summarised_t35$groupID <- allBehaviour1022_group_riskID12_summarised_t35$room


allBehaviour1022_indiv <- read.csv("allBehaviour1022_indiv.csv")
allBehaviour1022_indiv_riskID12 <- allBehaviour1022_indiv %>% filter(riskDistributionId_factor=='Con: 2') #'Con: 2' means the 2-risky-2-safe task
allBehaviour1022_indiv_riskID12$sub_old <- allBehaviour1022_indiv_riskID12$sub
allBehaviour1022_indiv_riskID12$sub <- allBehaviour1022_indiv_riskID12$amazonID %>% as.factor() %>% as.numeric()

allBehaviour1022_indiv_riskID12_summarised_t35 <- allBehaviour1022_indiv_riskID12 %>%
	dplyr::filter(round>35) %>%
	group_by(amazonID, sub) %>%
	summarise(
		risky_choice_count = sum(best_risky_choice, na.rm = TRUE),
		risky_choice_mean = mean(best_risky_choice, na.rm=TRUE),
		trial_num = n(),
		indivOrGroup_factor = indivOrGroup_factor[1],
		room = room[1]
	)
allBehaviour1022_indiv_riskID12_summarised_t35$groupID = allBehaviour1022_indiv_riskID12_summarised_t35$room
allBehaviour1022_indiv_riskID12_summarised_t35$groupID[which(allBehaviour1022_indiv_riskID12_summarised_t35$indivOrGroup_factor=='Individual')] = 'Individual'

# Individual fits
fit_AL00_multiVar_LKJ_indiv_riskID12_parameters <- read.csv('fit_AL00_multiVar_LKJ_indiv_riskID12_parameters.csv')

# Merging the behavioural data with the fit parameters
# hot stove effect - individual
fit_AL_indiv_riskID12_parameters <- right_join(fit_AL00_multiVar_LKJ_indiv_riskID12_parameters, allBehaviour1022_indiv_riskID12_summarised_t35, by = 'sub')
fit_AL_indiv_riskID12_parameters$hot_stove_susceptibility <- fit_AL_indiv_riskID12_parameters$alpha_median_AL00_multiVar_LKJ * (1+ fit_AL_indiv_riskID12_parameters$beta_median_AL00_multiVar_LKJ)
fit_AL_indiv_riskID12_parameters$hot_stove_susceptibility_trancated <- fit_AL_indiv_riskID12_parameters$hot_stove_susceptibility
fit_AL_indiv_riskID12_parameters$hot_stove_susceptibility_trancated[which(fit_AL_indiv_riskID12_parameters$hot_stove_susceptibility > 6)] <- 6

# hot stove effect - group
fit_SL00_riskID12_parameters <- right_join(fit_SL00_multiVar_LKJ_1022_parameters, allBehaviour1022_group_riskID12_summarised_t35, by = 'sub')
fit_SL00_riskID12_parameters$hot_stove_susceptibility <- fit_SL00_riskID12_parameters$alpha_mean_SL00_multiVar_LKJ * (1+ fit_SL00_riskID12_parameters$beta_mean_SL00_multiVar_LKJ)
fit_SL00_riskID12_parameters$hot_stove_susceptibility_trancated <- fit_SL00_riskID12_parameters$hot_stove_susceptibility
fit_SL00_riskID12_parameters$hot_stove_susceptibility_trancated[which(fit_SL00_riskID12_parameters$hot_stove_susceptibility > 6)] <- 6

# overall means
social_learning_model_validation_1022_riskID12_summary <-
	social_learning_model_validation_1022_riskID12_data %>%
	group_by(condition_dummy, hot_stove_susceptibility_rounded, soc_mean_category) %>%
	summarise(
		proportionRiskyChoice_b2_mean = mean(proportionRiskyChoice_b2),
		proportionRiskyChoice_b2_sd = sd(proportionRiskyChoice_b2),
		raw_proportionRiskyChoice_b2_mean = proportionRiskyChoice_b2 %>% convert_alpha_to_alphaRaw() %>% mean(),
		raw_proportionRiskyChoice_b2_sd = proportionRiskyChoice_b2 %>% convert_alpha_to_alphaRaw() %>% sd(),
		soc_mean = mean(soc_mean),
		n = n()
		)

social_learning_model_validation_1022_riskID12_summary$proportionRiskyChoice_b2_lower <-
	(social_learning_model_validation_1022_riskID12_summary$raw_proportionRiskyChoice_b2_mean - social_learning_model_validation_1022_riskID12_summary$raw_proportionRiskyChoice_b2_sd / sqrt(social_learning_model_validation_1022_riskID12_summary$n)) %>% convert_alphaRaw_to_alpha
social_learning_model_validation_1022_riskID12_summary$proportionRiskyChoice_b2_upper <-
	(social_learning_model_validation_1022_riskID12_summary$raw_proportionRiskyChoice_b2_mean + social_learning_model_validation_1022_riskID12_summary$raw_proportionRiskyChoice_b2_sd / sqrt(social_learning_model_validation_1022_riskID12_summary$n)) %>% convert_alphaRaw_to_alpha

social_learning_model_validation_1022_riskID12_summary$proportionRiskyChoice_b2_mid <-
	social_learning_model_validation_1022_riskID12_summary$raw_proportionRiskyChoice_b2_mean %>% convert_alphaRaw_to_alpha


# modest social learners' means
social_learning_model_validation_1022_riskID12_summary_reallyHighSigma <-
	social_learning_model_validation_1022_riskID12_data %>%
	dplyr::filter(soc_mean > 5/10 & soc_mean < 8/10 & hot_stove_susceptibility_rounded < 6) %>%
	group_by(condition_dummy, hot_stove_susceptibility_rounded, soc_mean_category) %>%
	summarise(
		proportionRiskyChoice_b2_mean = mean(proportionRiskyChoice_b2),
		proportionRiskyChoice_b2_sd = sd(proportionRiskyChoice_b2),
		raw_proportionRiskyChoice_b2_mean = proportionRiskyChoice_b2 %>% convert_alpha_to_alphaRaw() %>% median(),
		raw_proportionRiskyChoice_b2_sd = proportionRiskyChoice_b2 %>% convert_alpha_to_alphaRaw() %>% sd(),
		soc_mean = mean(soc_mean),
		n = n()
		)

social_learning_model_validation_1022_riskID12_summary_reallyHighSigma$proportionRiskyChoice_b2_mid <-
	social_learning_model_validation_1022_riskID12_summary_reallyHighSigma$raw_proportionRiskyChoice_b2_mean %>% convert_alphaRaw_to_alpha

```

## Plot (Figure 6)
```{r fig.width=12, fig.height=4.5}
ggplot() +
	geom_segment(aes(x=0,xend=5.8,y=0.5,yend=0.5),colour="grey30", size=0.5) +
  geom_ribbon(data=social_learning_model_validation_0820_summary%>%dplyr::filter(condition_dummy==0&hot_stove_susceptibility_rounded<6), mapping=aes(hot_stove_susceptibility_rounded, ymin=proportionRiskyChoice_b2_lower, ymax=proportionRiskyChoice_b2_upper), fill='grey20', alpha=1/2)+
	geom_ribbon(data=social_learning_model_validation_0820_summary%>%dplyr::filter(condition_dummy==1&soc_mean_category=='mild'&hot_stove_susceptibility_rounded<6), mapping=aes(hot_stove_susceptibility_rounded, ymin=proportionRiskyChoice_b2_lower, ymax=proportionRiskyChoice_b2_upper), fill='orange', alpha=1/2)+
  geom_point(data = parameterfit_indiv_AL00_0820, mapping=aes(hot_stove_susceptibility_trancated, risky_choice_mean), colour='grey20', shape = 17)+ # shape=5: diamond
	geom_point(data = fit_parameters_group_SL00_mcmc, mapping=aes(hot_stove_susceptibility_trancated,risky_choice_mean, colour=soc_mean), shape = 20) +
	geom_line(data=social_learning_model_validation_0820_summary%>%dplyr::filter(condition_dummy==0&hot_stove_susceptibility_rounded<6), mapping=aes(hot_stove_susceptibility_rounded, proportionRiskyChoice_b2_mid , linetype = "Ind"), size=1.0)+
	geom_line(data=social_learning_model_validation_0820_summary%>%dplyr::filter(condition_dummy==1&hot_stove_susceptibility_rounded<6), mapping=aes(hot_stove_susceptibility_rounded, proportionRiskyChoice_b2_mid, group=soc_mean_category, colour=mean(soc_mean) , linetype = "g1"), size=1.0)+
	geom_line(data=social_learning_model_validation_0820_summary_reallyHighSigma, mapping=aes(hot_stove_susceptibility_rounded, proportionRiskyChoice_b2_mid, group=soc_mean_category, colour=mean(soc_mean) , linetype = 'g2'), size=1.0)+
	scale_colour_viridis_c(expression('Copying weight \U03C3'[i]), begin = 0.2, end = 0.9, option='plasma', direction=-1)+
  scale_linetype_manual(name = ""
                 , breaks = c("Ind","g1","g2")
                 , labels = c("Individual","Group average","Group with \U03C3 = 0.4")
                 , values = c('solid','solid','dashed')
                 , guide = guide_legend(
                   override.aes = list(linetype = c('solid','solid','dashed')
                                       , size=0.6
                                       , color = c('black','#ff751a','#ff6666'))
                   , order=1)
                 ) +
	myTheme_Helvetica()+
	xlim(c(0,6.5))+
	labs(
		x = expression(atop('Susceptibility to the hot stove effect', paste(alpha[i], '(', beta[i], '+1)'))),
		y = 'Mean proportion of choosing\nthe optimal risky option',
		title = 'The 1-risky-1-safe task \n(N = 168)') +
  guides(colour = "none")+
	#theme(legend.position = c(0.85, 0.5))+
	#theme(legend.position = NaN)+
	theme(legend.position = c(0.65, 0.75))+
	theme(legend.title = element_text(size=12))+
	theme(legend.text = element_text(size=11))+
  theme(legend.key = element_rect(fill = "transparent", colour = "transparent"))+
	NULL -> fig6_a

ggplot() +
	geom_segment(aes(x=0,xend=6,y=0.25,yend=0.25),colour="grey30", size=0.5) +
  geom_ribbon(data=social_learning_model_validation_1022_riskID11_summary%>%dplyr::filter(condition_dummy==0), mapping=aes(hot_stove_susceptibility_rounded, ymin=proportionRiskyChoice_b2_lower, ymax=proportionRiskyChoice_b2_upper), fill='grey20', alpha=1/2)+
	geom_ribbon(data=social_learning_model_validation_1022_riskID11_summary%>%dplyr::filter(condition_dummy==1&soc_mean_category=='mild'), mapping=aes(hot_stove_susceptibility_rounded, ymin=proportionRiskyChoice_b2_lower, ymax=proportionRiskyChoice_b2_upper), fill='orange', alpha=1/2)+
	geom_point(data = fit_AL_indiv_riskID11_parameters, mapping=aes(hot_stove_susceptibility_trancated, risky_choice_mean), colour='grey20', shape = 17)+ # shape=5: diamond
	geom_point(data = fit_SL00_riskID11_parameters, mapping=aes(hot_stove_susceptibility_trancated,risky_choice_mean, colour=soc_mean_SL00_multiVar_LKJ), shape = 20) +
	geom_line(data=social_learning_model_validation_1022_riskID11_summary%>%dplyr::filter(condition_dummy==0), mapping=aes(hot_stove_susceptibility_rounded, proportionRiskyChoice_b2_mid), size=1.0)+
	geom_line(data=social_learning_model_validation_1022_riskID11_summary%>%dplyr::filter(condition_dummy==1), mapping=aes(hot_stove_susceptibility_rounded, proportionRiskyChoice_b2_mid, group=soc_mean_category, colour=mean(soc_mean)), size=1.0)+
	geom_line(data=social_learning_model_validation_1022_riskID11_summary_reallyHighSigma, mapping=aes(hot_stove_susceptibility_rounded, proportionRiskyChoice_b2_mid, group=soc_mean_category, colour=mean(soc_mean)), linetype = 'dashed', size=1.0)+
	scale_colour_viridis_c(expression('Copying weight \U03C3'[i]), begin = 0.2, end = 0.9, option='plasma', direction=-1)+
	myTheme_Helvetica()+
	xlim(c(0,6.5))+
	labs(
		x = expression(atop('Susceptibility to the hot stove effect', paste(alpha[i], '(', beta[i], '+1)'))),
		y = 'Mean proportion of choosing\nthe optimal risky option',
		title = 'The 1-risky-3-safe task \n(N = 148)') +
	theme(legend.position = NaN)+
	theme(legend.title = element_text(size=12))+
	theme(legend.text = element_text(size=11))+
	NULL -> fig6_b

ggplot() +
	geom_segment(aes(x=0,xend=6,y=0.25,yend=0.25),colour="grey30", size=0.5) +
  geom_ribbon(data=social_learning_model_validation_1022_riskID12_summary%>%dplyr::filter(condition_dummy==0), mapping=aes(hot_stove_susceptibility_rounded, ymin=proportionRiskyChoice_b2_lower, ymax=proportionRiskyChoice_b2_upper), fill='grey20', alpha=1/2)+
	geom_ribbon(data=social_learning_model_validation_1022_riskID12_summary%>%dplyr::filter(condition_dummy==1&soc_mean_category=='mild'), mapping=aes(hot_stove_susceptibility_rounded, ymin=proportionRiskyChoice_b2_lower, ymax=proportionRiskyChoice_b2_upper), fill='orange', alpha=1/2)+
	geom_point(data = fit_AL_indiv_riskID12_parameters, mapping=aes(hot_stove_susceptibility_trancated, risky_choice_mean), colour='grey20', shape = 17)+ # shape=5: diamond
  geom_point(data = fit_SL00_riskID12_parameters, mapping=aes(hot_stove_susceptibility_trancated,risky_choice_mean, colour=soc_mean_SL00_multiVar_LKJ), shape = 20) +
	geom_line(data=social_learning_model_validation_1022_riskID12_summary%>%dplyr::filter(condition_dummy==0), mapping=aes(hot_stove_susceptibility_rounded, proportionRiskyChoice_b2_mid), size=1.0)+
	geom_line(data=social_learning_model_validation_1022_riskID12_summary%>%dplyr::filter(condition_dummy==1), mapping=aes(hot_stove_susceptibility_rounded, proportionRiskyChoice_b2_mid, group=soc_mean_category, colour=mean(soc_mean)), size=1.0)+
	geom_line(data=social_learning_model_validation_1022_riskID12_summary_reallyHighSigma, mapping=aes(hot_stove_susceptibility_rounded, proportionRiskyChoice_b2_mid, group=soc_mean_category, colour=mean(soc_mean)), linetype = "dashed", size=1.0)+
	scale_colour_viridis_c(expression('Copying weight \U03C3'[i]), begin = 0.2, end = 0.9, option='plasma', direction=-1)+
	myTheme_Helvetica()+
	xlim(c(0,6.5))+
	labs(
		x = expression(atop('Susceptibility to the hot stove effect', paste(alpha[i], '(', beta[i], '+1)'))),
		y = 'Mean proportion of choosing\nthe optimal risky option',
		title = 'The 2-risky-2-safe task \n(N = 151)') +
	theme(legend.position = c(0.75, 0.7))+
	theme(legend.title = element_text(size=12))+
	theme(legend.text = element_text(size=11))+
	NULL -> fig6_c

(
  figure_exp_model_pred <- ggarrange(fig6_a, fig6_b, fig6_c
                                     # ,  common.legend = TRUE
                                     # # ,  legend = 'right'
                                     , labels = c('a','b','c')
                                     , ncol = 3, align = 'v'
	)
)

ggsave(file = "figure_exp_model_pred.pdf"
       , plot = figure_exp_model_pred
       , dpi = 600, width = 12, height = 4.5
       , device = cairo_pdf
	)
```
  