###############################################################################################
##
## Exploration of individual reinforcement learning parameters that give risk aversion or risk seeking
## Using tasks with a risky choice whose premium is 1.5
## (i.e. expected value is 1.5 times as high as safe option)
## Wataru Toyokawa
## 30 March. 2020
##
###############################################################################################

## Assumptions

## - Rescorla-Wagner rule with time-fixed alpha
## - Softmax choice with fixed inverse temperature (no annealing; i.e. beta = beta_0 + 0 * t/T)
## - Individual differences in the learning parameters (see Toyokawa et al. (2019) Nat. Hum. Behav.)
## - payoff is generated by a Gaussian probability distribution with standard deviation sigma

rm(list=ls(all=TRUE)) # cleaning the workspace

# Loading
library(readr)
library(tidyverse)
library(ggplot2)
library(cowplot)
# library(ggjoy)
library(foreach)
library(MASS)
library(doParallel)
registerDoParallel(detectCores())

## Load Functions
source('~/Dropbox/wataru/papers/RiskySocialLearning/riskPrem1.5_Gaussian/functions.R')

## -- simulated model --
model = 'Decision-Biasing'

## -- Global parameter setting --
sigmaList = c(0, 0.25, 0.5, 0.75)
thetaList = c(1, 2, 4)
alphaList = seq(0.01,0.96,0.05)
invTemperatureList = seq(0,10,1)
## -- debug lists --
#sigmaList = c(0, 0.1, 0.2, 0.3)
#thetaList = c(1, 2)
#alphaList = c(0.1, 0.3, 0.8)
#invTemperatureList = c(1,3,5)
## -- debug lists END --
annealing = 0
groupSizeList = c(10)
repetition = 1000
horizon = 150 # = number of trials
numOptions = 2

## -- transformed parameters
alphaRawList = mapply(convert_alpha_to_alphaRaw, alphaList)
sigmaRawList = mapply(convert_alpha_to_alphaRaw, sigmaList)

## -- Individual variation
variationAlphaRaw = 0.01 #1.5
variationBeta = 0.01#0.7
variatonAnnealing = 0.01#1.5
variationSigmaRaw = 0.01
variationTheta = 0.01

# -- Setting a two-armed bandit task --
riskPremium = 1.5
mu_sure = 1
mu_risky = mu_sure * riskPremium
sd_sure = 0.03
sd_risky = 1
initialExpextation = 0

# -- Initial Settings --
socialLearningParamSearch_riskPrem150_Gaussian_indivDiff = list()
groupSize = groupSizeList[1]

sigmaFlag = 0
# -- simulation --
s_time = Sys.time()
for (theta in thetaList) {
	for (sigma in sigmaList) {
		if (sigmaFlag==1&sigma==0) next
		for (alpha in alphaList) {
			for (invTemperature in invTemperatureList) {
				socialLearningParamSearch_riskPrem150_Gaussian_indivDiff[[paste("n=", groupSize)]][[paste("alpha=", alpha)]][[paste("invTemperature=", invTemperature)]][[paste("sigma=", sigma)]][[paste("theta=", theta)]] <- foreach(rep = 1:repetition, .combine=rbind) %dopar% {
					## Initial settings
					choices = matrix(nrow=groupSize, ncol=horizon)
					payoffs = matrix(nrow=groupSize, ncol=horizon)
					performance = matrix(nrow=groupSize, ncol=horizon)
					safeChoiceProb = matrix(nrow=groupSize, ncol=horizon)
					isThisBestOption = matrix(nrow=groupSize, ncol=horizon)
					optimalChoiceProb = matrix(nrow=groupSize, ncol=horizon)
					expectedPerformance = matrix(nrow=groupSize, ncol=horizon)
					Q = array(dim = c(numOptions, horizon, groupSize))
					choiceCounter = array(1, dim = c(numOptions, groupSize))
					netChoiceProb = array(dim = c(numOptions, horizon, groupSize))
					netChoiceProb[,1,] = 1/numOptions
					Q[,1,] = initialExpextation
					socialFrequency = matrix(nrow=numOptions, ncol=horizon)
					socialFrequency[,] = 1e-1
					## Setting individual parameters
					thisAlpha = ( alphaRawList[which(alphaList==alpha)] + variationAlphaRaw * rt(groupSize, df = 14, ncp = 0) ) %>% mapply(convert_alphaRaw_to_alpha, .)
					thisBeta = invTemperature + variationBeta  * rt(groupSize, df = 14, ncp = 0)
					thisBeta[which(thisBeta<0)] <- 0
					#thisAnnealing = annealing + variatonAnnealing * rt(groupSize, df = 14, ncp = 0)
					if(sigma!=0){
						thisSigma = ( sigmaRawList[which(sigmaList==sigma)] + variationSigmaRaw * rt(groupSize, df = 14, ncp = 0) ) %>% mapply(convert_alphaRaw_to_alpha, .)
					}else{
						thisSigma <- rep(0, groupSize)
					}
					thisTheta = theta + variationTheta * rt(groupSize, df = 14, ncp = 0)
					thisAnnealing = 0
					for(t in 1:horizon){
						# each individual chooses one option based on his/her choice probability
						choices[,t] = mapply(function(p1,p2){ sample(1:numOptions, 1, prob=c(p1,p2), replace=FALSE) }, netChoiceProb[1,t,], netChoiceProb[2,t,] )
						# each subject earns some money (if lucky)
						payoffs[,t] = payoffGenerateGaussian(groupSize, choices[,t], mu_sure, mu_risky, sd_sure, sd_risky)
						# update choiceCounter and learningRate (if the learning rate is an averaging rule in this simulation.)
						updatingPositions = (choices[,t] + numOptions*(1:groupSize-1))
						# -- (if the learning rate is an averaging rule in this simulation.) --
						#choiceCounter = aperm(choiceCounter, c(2,1)) # transform the choiceCounter matrix
						#dim(choiceCounter) = c(numOptions*groupSize) # reduce dimension
						#choiceCounter[updatingPositions] = choiceCounter[updatingPositions] + 1
						#learningRate = 1/choiceCounter # Learning rate is now set
						#dim(choiceCounter) = c(numOptions, groupSize)
						# -- END --
						if(t < horizon) {
							if(t == 1) {
								Q[,t+1,] = Q[,t,]
								QQ = aperm(Q, c(1,3,2))
								dim(QQ) = c(numOptions*groupSize, horizon)
								# In the first trial, all Q values are updated by the first experience
								QQ[,t+1] = QQ[,t] + thisAlpha * (payoffs[,t] - QQ[,t])
								dim(QQ) = c(numOptions, groupSize, horizon)
								Q = aperm(QQ, c(1,3,2))
							} else {
								# Updating Q value based on Rescorla-Wagner model (Weighted return model)
								Q[,t+1,] = Q[,t,]
								QQ = aperm(Q, c(1,3,2))
								dim(QQ) = c(numOptions*groupSize, horizon)
								#QQ[updatingPositions,t+1] = QQ[updatingPositions,t] + learningRate[updatingPositions] * (payoffs[,t] - QQ[updatingPositions,t])
								QQ[updatingPositions,t+1] = QQ[updatingPositions,t] + thisAlpha * (payoffs[,t] - QQ[updatingPositions,t])
								dim(QQ) = c(numOptions, groupSize, horizon)
								Q = aperm(QQ, c(1,3,2))
							}
							# update socialFrequency
							## Option 1's frequency
							if(length(which(names(table(choices[,t]))==1))>0) {
								socialFrequency[1,t+1] = socialFrequency[1,t+1] + table(choices[,t])[which(names(table(choices[,t]))==1)][1]
							}
							## Option 2's frequency
							if(length(which(names(table(choices[,t]))==2))>0) {
								socialFrequency[2,t+1] = socialFrequency[2,t+1] + table(choices[,t])[which(names(table(choices[,t]))==2)][1]
							}
							# Calculating the next choice probability
							# It depends on what strategy each agent deploys
							# In the original article, March only considered a proportional choice
							# If you want to implement softmax rule, you should modify this
							#proportionalChoiceMatrix = Q[,t+1,] %>% apply(1, divideVector, denominator = apply(Q[,t+1,],2,sum)) %>% t()

							###############
							## Softmax choice base solely on Q values
							###############
							#Q_exp = apply(Q[,t+1,], 1, multiplyBeta, beta = (thisBeta + thisAnnealing * (t+1)/horizon) ) %>% t() %>% apply(2,expCeiling)
							Q_exp = ( Q[,t+1,] * rep((thisBeta + thisAnnealing * (t+1)/horizon), each = numOptions) ) %>% apply(2,expCeiling)
							softmaxMatrix = Q_exp %>% apply(1, divideVector, denominator = apply(Q_exp,2,sum)) %>% t()
							freqDepenMatrix = frequencyDependentCopy(socialFrequency[,t+1], choices[,t], thisTheta, numOptions)
							## The followings update the choice probability matrix
							###############
							## Softmax -- END
							###############

							#soc = 1/(1+exp(-(soc_raw)))
							#soc = 0 # soc = 0 indicates asocial learning (i.e. no social info use)
							##netMatrix = apply(softmaxMatrix, 1, multiplyBeta, beta=(1-epsilon*numOptions)) %>% t() + epsilon
							#netMatrix = apply(softmaxMatrix, 1, multiplyBeta, beta=(1-soc)) %>% t() + apply(freqDepenMatrix, 1, multiplyBeta, beta=soc) %>% t()
							if(model=='Decision-Biasing'){
								netMatrix = apply(softmaxMatrix, 1, multiplyBeta, beta=(1-thisSigma)) %>% t() + apply(freqDepenMatrix, 1, multiplyBeta, beta=thisSigma) %>% t()
							}else{
								netMatrix = softmaxMatrix
							}
							netChoiceProbAperm = aperm(netChoiceProb, c(1,3,2))
							dim(netChoiceProbAperm) = c(numOptions*groupSize, horizon)
							dim(netMatrix) = c(numOptions*groupSize, 1)
							netChoiceProbAperm[,t+1] = netMatrix
							dim(netChoiceProbAperm) = c(numOptions, groupSize, horizon)
							netChoiceProb = aperm(netChoiceProbAperm, c(1,3,2))
						}
					}

					for(i in 1:groupSize) {
						safeChoiceProb[i,] = netChoiceProb[1,,i]
					}

					safeChoiceProbMean = safeChoiceProb[,(horizon/2+1):horizon] %>% mean()

					# Submitting this repetition's result
					print(
						c(
							groupSize,
							alpha,
							invTemperature,
							sigma,
							theta,
							safeChoiceProbMean
						)
					)
				}
				if(sigma==0) sigmaFlag <- 1
				gc();gc() # rubbish collection
			}
		}
	}
}
e_time = Sys.time()
e_time - s_time
# -- simulation END --

# -- saving the data --
socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_data = socialLearningParamSearch_riskPrem150_Gaussian_indivDiff[[paste("n=",groupSizeList[1])]][[paste("alpha=",alphaList[1])]][[paste("invTemperature=",invTemperatureList[1])]][[paste("sigma=",sigmaList[1])]][[paste("theta=",thetaList[1])]] %>% data.frame()

for(theta in thetaList) {
	for(sigma in sigmaList) {
		for (groupSize in groupSizeList) {
			for (alpha in alphaList) {
				for (invTemperature in invTemperatureList) {
					if(groupSize!=groupSizeList[1] | alpha!=alphaList[1] | invTemperature!=invTemperatureList[1]| sigma!=sigmaList[1] | theta!=thetaList[1]) {
						socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_data = socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_data %>% rbind(data.frame(socialLearningParamSearch_riskPrem150_Gaussian_indivDiff[[paste("n=",groupSize)]][[paste("alpha=",alpha)]][[paste("invTemperature=",invTemperature)]][[paste("sigma=",sigma)]][[paste("theta=",theta)]]))
					}
				}
			}
		}
	}
}

names(socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_data) = c('groupSize', 'learningRate', 'invTemperature', 'copyRate', 'conformityExp', 'proportionSafeChoice')


socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_summary <- socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_data %>%
	group_by(groupSize, learningRate, invTemperature, copyRate, conformityExp) %>%
	summarise(
		mean_proportionSafeChoice = mean(proportionSafeChoice, na.rm = TRUE),
	    median_proportionSafeChoice = median(proportionSafeChoice, na.rm = TRUE),
	    sd_proportionSafeChoice = sd(proportionSafeChoice, na.rm = TRUE)
		)

## Adding Denrell (2007)'s analytical solution
Denrell2007 = function (alpha, beta, mu, sd) {
	1 / ( 1 + exp( (alpha*(beta^2)*(sd^2))/(2*(2-alpha)) - beta*mu ) )
}

## when mu = surePayoff + 0.5 and sd = 1, the above function can be reduced as follows:
Denrell2007Solution = function (alpha) {
	(2 - alpha)/alpha
}

Denrell2007RiskyChoice = c()
alphaArray = c()
betaArray = c()
for (alpha in seq(0,1,0.1)) {
	for (beta in seq(0,10,1)) {
		alphaArray <- append(alphaArray, alpha)
		betaArray <- append(betaArray, beta)
		Denrell2007RiskyChoice <- append(Denrell2007RiskyChoice, Denrell2007(alpha, beta, mu=0.5, sd=1))
	}
}

Denrell2007Simulation = data.frame(alpha = alphaArray, beta = betaArray, riskyChoiceProb = Denrell2007RiskyChoice)

Denrell2007Simulation %>%
	ggplot(aes(alpha, beta))+
	geom_raster(aes(fill = riskyChoiceProb), stat = 'identity')+
	stat_function(fun=Denrell2007Solution, color='black', linetype='dashed', size=2/3)+
	scale_fill_gradient2(midpoint = 0.5, low = "blue", mid = "grey90", high = "red")+
	myTheme_gillsansMT()+
	ylim(c(0,10))+
	NULL


socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_summary$copyRate_factor = paste(rep('c = ', nrow(socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_summary)), socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_summary$copyRate, sep ='')

socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_summary$conformityExp_factor = paste(rep('f = ', nrow(socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_summary)), socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_summary$conformityExp, sep ='')

# plot with Denrell (2007) curve
socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_summary %>%
	ggplot() +
	geom_raster(mapping = aes(learningRate, invTemperature, fill = mean_proportionSafeChoice), stat = 'identity') +
	labs(x=expression(paste('Learning rate ',alpha,sep="")), 
		y=expression(paste('Inverse temperature ',beta,sep="")), 
		#title='Gaussian noise\n mu=0.5; sigma=1', 
		fill = "Proportion of\nsafe choice"
		#title='Gaussian noise\n mu=0.5; sigma=1', fill = "Proportion of\nsafe choice"
		)+
	#scale_fill_viridis(limits = c(0.45, 1), option="magma")+
	stat_function(fun=Denrell2007Solution, color='black', linetype='dashed', size=2/3)+
	scale_fill_gradient2(midpoint = 0.5, low = "red", mid = "grey90", high = "blue")+
	ylim(c(0,10))+
	#myTheme_gillsansMT()+
	myTheme_Times()+
	theme(axis.text.x = element_text(angle = 90))+
	theme(strip.text.y = element_text(angle = 0))+
	theme(legend.text = element_text(angle = 45))+
	theme(legend.position = 'top')+
	facet_grid(copyRate_factor ~ conformityExp_factor)+
	NULL -> socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_plot_mean
socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_plot_mean


socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_summary %>%
	ggplot() +
	geom_raster(mapping = aes(learningRate, invTemperature, fill = sd_proportionSafeChoice), stat = 'identity') +
	#labs(x=expression(paste('Learning rate ',bar(alpha),sep="")), y=expression(paste('Inverse temperature ',bar(beta),sep="")), title='Gaussian noise\n mu=0.5; sigma=1', fill="SD of\nsafe choice rate")+
	labs(x=expression(paste('Learning rate ',alpha,sep="")), y=expression(paste('Inverse temperature ',beta,sep="")), title='Gaussian noise\n mu=0.5; sigma=1', fill="SD of\nsafe choice rate")+
	scale_fill_viridis_c(option="magma")+
	#scale_fill_gradient2(midpoint = 0.5, low = "red", mid = "grey90", high = "blue")+
	#myTheme_gillsansMT()+
	myTheme_Times()+
	theme(axis.text.x = element_text(angle = 90))+
	facet_grid(copyRate ~ conformityExp)+
	NULL -> socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_plot_sd
socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_plot_sd

ggsave(file = "~/Dropbox/wataru/papers/RiskySocialLearning/riskPrem1.5_Gaussian/results/socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_plot_mean.png", plot = socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_plot_mean, dpi = 300, width = 9, height = 7)
ggsave(file = "~/Dropbox/wataru/papers/RiskySocialLearning/riskPrem1.5_Gaussian/results/socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_plot_sd.png", plot = socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_plot_sd, dpi = 300, width = 9, height = 8)


write.csv(socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_data,
			"~/Dropbox/wataru/papers/RiskySocialLearning/riskPrem1.5_Gaussian/results/socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_data.csv",
			row.names=FALSE)

socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_data <- read_csv("~/Dropbox/wataru/papers/RiskySocialLearning/riskPrem1.5_Gaussian/results/socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_data.csv")
socialLearningParamSearch_riskPrem150_Gaussian_indivDiff_data$learningRate = rep(alphaList, each = length(invTemperatureList)*repetition) %>% rep(length(groupSizeList))
