###############################################################################################
##
## Exploration of individual reinforcement learning parameters that give risk aversion or risk seeking
## Using tasks with a risky choice whose premium is 1.5
## (i.e. expected value is 1.5 times as high as safe option)
## Wataru Toyokawa
## 30 March. 2020
##
###############################################################################################

## Assumptions

## - Rescorla-Wagner rule with time-fixed alpha
## - Softmax choice with fixed inverse temperature (no annealing; i.e. beta = beta_0 + 0 * t/T)
## - Individual differences in the learning parameters (see Toyokawa et al. (2019) Nat. Hum. Behav.)
## - payoff is generated by a Gaussian probability distribution with standard deviation sigma

rm(list=ls(all=TRUE)) # cleaning the workspace

# Loading
# library(readr)
library(tidyverse)
# library(ggplot2)
library(cowplot)
# library(ggjoy)
library(foreach)
library(MASS)
library(doParallel)
registerDoParallel(detectCores())

## Load Functions
source('~/Dropbox/wataru/papers/RiskySocialLearning/riskPrem1.5_Gaussian/functions.R')

## -- simulated model --
model = 'Decision-Biasing'

## -- Global parameter setting --
sigmaList = c(0, 0.25, 0.5)
thetaList = c(1, 4)
alphaList = seq(0.01,0.96,0.05)
invTemperatureList = c(3,5,7)
## -- debug lists --
#sigmaList = c(0, 0.1, 0.2, 0.3)
#thetaList = c(1, 2)
#alphaList = c(0.1, 0.3, 0.8)
#invTemperatureList = c(1,3,5)
## -- debug lists END --
annealing = 0
groupSizeList = c(10)
repetition = 500 # 1000
horizon = 1075 # = number of trials
numOptions = 2

## -- transformed parameters
alphaRawList = mapply(convert_alpha_to_alphaRaw, alphaList)
sigmaRawList = mapply(convert_alpha_to_alphaRaw, sigmaList)

## -- Individual variation
variationAlphaRaw = 0.01 #1.5
variationBeta = 0.01#0.7
variatonAnnealing = 0.01#1.5
variationSigmaRaw = 0.01
variationTheta = 0.01

# -- Setting a two-armed bandit task --
riskPremium = 1.5
mu_sure = 1
mu_risky = mu_sure * riskPremium
sd_sure = 0.03
sd_risky = 1
initialExpextation = 0

# -- Initial Settings --
fig1_alt_long = list()
groupSize = groupSizeList[1]

sigmaFlag = 0
# -- simulation --
s_time = Sys.time()
for (theta in thetaList) {
	for (sigma in sigmaList) {
		if (sigmaFlag==1&sigma==0) next
		for (alpha in alphaList) {
			for (invTemperature in invTemperatureList) {
				fig1_alt_long[[paste("n=", groupSize)]][[paste("alpha=", alpha)]][[paste("invTemperature=", invTemperature)]][[paste("sigma=", sigma)]][[paste("theta=", theta)]] <- foreach(rep = 1:repetition, .combine=rbind) %dopar% {
					## Initial settings
					choices = matrix(nrow=groupSize, ncol=horizon)
					payoffs = matrix(nrow=groupSize, ncol=horizon)
					performance = matrix(nrow=groupSize, ncol=horizon)
					safeChoiceProb = matrix(nrow=groupSize, ncol=horizon)
					isThisBestOption = matrix(nrow=groupSize, ncol=horizon)
					optimalChoiceProb = matrix(nrow=groupSize, ncol=horizon)
					expectedPerformance = matrix(nrow=groupSize, ncol=horizon)
					Q = array(dim = c(numOptions, horizon, groupSize))
					choiceCounter = array(1, dim = c(numOptions, groupSize))
					netChoiceProb = array(dim = c(numOptions, horizon, groupSize))
					netChoiceProb[,1,] = 1/numOptions
					Q[,1,] = initialExpextation
					socialFrequency = matrix(nrow=numOptions, ncol=horizon)
					socialFrequency[,] = 1e-1
					## Setting individual parameters
					thisAlpha = ( alphaRawList[which(alphaList==alpha)] + variationAlphaRaw * rt(groupSize, df = 14, ncp = 0) ) %>% mapply(convert_alphaRaw_to_alpha, .)
					thisBeta = invTemperature + variationBeta  * rt(groupSize, df = 14, ncp = 0)
					thisBeta[which(thisBeta<0)] <- 0
					#thisAnnealing = annealing + variatonAnnealing * rt(groupSize, df = 14, ncp = 0)
					if(sigma!=0){
						thisSigma = ( sigmaRawList[which(sigmaList==sigma)] + variationSigmaRaw * rt(groupSize, df = 14, ncp = 0) ) %>% mapply(convert_alphaRaw_to_alpha, .)
					}else{
						thisSigma <- rep(0, groupSize)
					}
					thisTheta = theta + variationTheta * rt(groupSize, df = 14, ncp = 0)
					thisAnnealing = 0
					for(t in 1:horizon){
						# each individual chooses one option based on his/her choice probability
						choices[,t] = mapply(function(p1,p2){ sample(1:numOptions, 1, prob=c(p1,p2), replace=FALSE) }, netChoiceProb[1,t,], netChoiceProb[2,t,] )
						# each subject earns some money (if lucky)
						payoffs[,t] = payoffGenerateGaussian(groupSize, choices[,t], mu_sure, mu_risky, sd_sure, sd_risky)
						# update choiceCounter and learningRate (if the learning rate is an averaging rule in this simulation.)
						updatingPositions = (choices[,t] + numOptions*(1:groupSize-1))
						# -- (if the learning rate is an averaging rule in this simulation.) --
						#choiceCounter = aperm(choiceCounter, c(2,1)) # transform the choiceCounter matrix
						#dim(choiceCounter) = c(numOptions*groupSize) # reduce dimension
						#choiceCounter[updatingPositions] = choiceCounter[updatingPositions] + 1
						#learningRate = 1/choiceCounter # Learning rate is now set
						#dim(choiceCounter) = c(numOptions, groupSize)
						# -- END --
						if(t < horizon) {
							if(t == 1) {
								Q[,t+1,] = Q[,t,]
								QQ = aperm(Q, c(1,3,2))
								dim(QQ) = c(numOptions*groupSize, horizon)
								# In the first trial, all Q values are updated by the first experience
								QQ[,t+1] = QQ[,t] + thisAlpha * (payoffs[,t] - QQ[,t])
								dim(QQ) = c(numOptions, groupSize, horizon)
								Q = aperm(QQ, c(1,3,2))
							} else {
								# Updating Q value based on Rescorla-Wagner model (Weighted return model)
								Q[,t+1,] = Q[,t,]
								QQ = aperm(Q, c(1,3,2))
								dim(QQ) = c(numOptions*groupSize, horizon)
								#QQ[updatingPositions,t+1] = QQ[updatingPositions,t] + learningRate[updatingPositions] * (payoffs[,t] - QQ[updatingPositions,t])
								QQ[updatingPositions,t+1] = QQ[updatingPositions,t] + thisAlpha * (payoffs[,t] - QQ[updatingPositions,t])
								dim(QQ) = c(numOptions, groupSize, horizon)
								Q = aperm(QQ, c(1,3,2))
							}
							# update socialFrequency
							## Option 1's frequency
							if(length(which(names(table(choices[,t]))==1))>0) {
								socialFrequency[1,t+1] = socialFrequency[1,t+1] + table(choices[,t])[which(names(table(choices[,t]))==1)][1]
							}
							## Option 2's frequency
							if(length(which(names(table(choices[,t]))==2))>0) {
								socialFrequency[2,t+1] = socialFrequency[2,t+1] + table(choices[,t])[which(names(table(choices[,t]))==2)][1]
							}
							# Calculating the next choice probability
							# It depends on what strategy each agent deploys
							# In the original article, March only considered a proportional choice
							# If you want to implement softmax rule, you should modify this
							#proportionalChoiceMatrix = Q[,t+1,] %>% apply(1, divideVector, denominator = apply(Q[,t+1,],2,sum)) %>% t()

							###############
							## Softmax choice base solely on Q values
							###############
							#Q_exp = apply(Q[,t+1,], 1, multiplyBeta, beta = (thisBeta + thisAnnealing * (t+1)/horizon) ) %>% t() %>% apply(2,expCeiling)
							Q_exp = ( Q[,t+1,] * rep((thisBeta + thisAnnealing * (t+1)/horizon), each = numOptions) ) %>% apply(2,expCeiling)
							softmaxMatrix = Q_exp %>% apply(1, divideVector, denominator = apply(Q_exp,2,sum)) %>% t()
							freqDepenMatrix = frequencyDependentCopy(socialFrequency[,t+1], choices[,t], thisTheta, numOptions)
							## The followings update the choice probability matrix
							###############
							## Softmax -- END
							###############

							#soc = 1/(1+exp(-(soc_raw)))
							#soc = 0 # soc = 0 indicates asocial learning (i.e. no social info use)
							##netMatrix = apply(softmaxMatrix, 1, multiplyBeta, beta=(1-epsilon*numOptions)) %>% t() + epsilon
							#netMatrix = apply(softmaxMatrix, 1, multiplyBeta, beta=(1-soc)) %>% t() + apply(freqDepenMatrix, 1, multiplyBeta, beta=soc) %>% t()
							if(model=='Decision-Biasing'){
								netMatrix = apply(softmaxMatrix, 1, multiplyBeta, beta=(1-thisSigma)) %>% t() + apply(freqDepenMatrix, 1, multiplyBeta, beta=thisSigma) %>% t()
							}else{
								netMatrix = softmaxMatrix
							}
							netChoiceProbAperm = aperm(netChoiceProb, c(1,3,2))
							dim(netChoiceProbAperm) = c(numOptions*groupSize, horizon)
							dim(netMatrix) = c(numOptions*groupSize, 1)
							netChoiceProbAperm[,t+1] = netMatrix
							dim(netChoiceProbAperm) = c(numOptions, groupSize, horizon)
							netChoiceProb = aperm(netChoiceProbAperm, c(1,3,2))
						}
					}

					for(i in 1:groupSize) {
						safeChoiceProb[i,] = netChoiceProb[1,,i]
					}

					safeChoiceProbMean = safeChoiceProb[,(horizon-75):horizon] %>% mean()

					# Submitting this repetition's result
					print(
						c(
							groupSize,
							alpha,
							invTemperature,
							sigma,
							theta,
							safeChoiceProbMean
						)
					)
				}
				if(sigma==0) sigmaFlag <- 1
				gc();gc() # rubbish collection
			}
		}
	}
}
e_time = Sys.time()
e_time - s_time
# -- simulation END --

# -- saving the data --
fig1_alt_long_data = fig1_alt_long[[paste("n=",groupSizeList[1])]][[paste("alpha=",alphaList[1])]][[paste("invTemperature=",invTemperatureList[1])]][[paste("sigma=",sigmaList[1])]][[paste("theta=",thetaList[1])]] %>% data.frame()

for(theta in thetaList) {
	for(sigma in sigmaList) {
		for (groupSize in groupSizeList) {
			for (alpha in alphaList) {
				for (invTemperature in invTemperatureList) {
					if(groupSize!=groupSizeList[1] | alpha!=alphaList[1] | invTemperature!=invTemperatureList[1]| sigma!=sigmaList[1] | theta!=thetaList[1]) {
						fig1_alt_long_data = fig1_alt_long_data %>% rbind(data.frame(fig1_alt_long[[paste("n=",groupSize)]][[paste("alpha=",alpha)]][[paste("invTemperature=",invTemperature)]][[paste("sigma=",sigma)]][[paste("theta=",theta)]]))
					}
				}
			}
		}
	}
}

names(fig1_alt_long_data) = c('groupSize', 'learningRate', 'invTemperature', 'copyRate', 'conformityExp', 'proportionSafeChoice')


write.csv(fig1_alt_long_data,
			"~/Dropbox/wataru/papers/RiskySocialLearning/riskPrem1.5_Gaussian/results/fig1_alt_long_data.csv",
			row.names=FALSE)


# figures of the simulation
#fig1_alt_long_data <- read_csv("~/Dropbox/wataru/papers/RiskySocialLearning/riskPrem1.5_Gaussian/results/fig1_alt_long_data.csv")

fig1_alt_long_summary <- fig1_alt_long_data %>%
	group_by(groupSize, learningRate, invTemperature, copyRate, conformityExp) %>%
	summarise(
		mean_proportionSafeChoice = mean(proportionSafeChoice, na.rm = TRUE),
	    median_proportionSafeChoice = median(proportionSafeChoice, na.rm = TRUE),
	    sd_proportionSafeChoice = sd(proportionSafeChoice, na.rm = TRUE)
		)


fig1_alt_long_summary$copyRate_factor = paste(rep('\U03C3 = ', nrow(fig1_alt_long_summary)), fig1_alt_long_summary$copyRate, sep ='')

fig1_alt_long_summary$conformityExp_factor = paste(rep('\U03b8 = ', nrow(fig1_alt_long_summary)), fig1_alt_long_summary$conformityExp, sep ='')

fig1_alt_long_summary$invTemperature_factor = paste(rep('\U03b2 = ', nrow(fig1_alt_long_summary)), fig1_alt_long_summary$invTemperature, sep ='')

fig1_alt_long_summary$hot_stove_suceptibility <- fig1_alt_long_summary$learningRate * (fig1_alt_long_summary$invTemperature + 1)

Pr_when_beta = function (X, beta) {
	Z = -beta/(2*(X/(beta+1))-2) * (X - 2)
	return_vector <- 1 / (1 + exp(Z))
	return_vector[which(X >= beta+1)] <- NA
	return_vector[which(X == 0)] <- 1/2
	return(return_vector)
	# if(X <= beta) {
	# 	1 / (1 + exp(Z))
	# }else{
	# 	return(0)
	# }
}

fig1_alt_long_summary_added <- fig1_alt_long_summary %>% rbind(fig1_alt_long_summary %>% dplyr::filter(copyRate==0))

added_length <- fig1_alt_long_summary %>% dplyr::filter(copyRate==0) %>% nrow()
all_length <- fig1_alt_long_summary_added %>% nrow()
fig1_alt_long_summary_added$conformityExp[(all_length-added_length+1):all_length] <- 4
fig1_alt_long_summary_added$conformityExp_factor[(all_length-added_length+1):all_length] <- '\U03b8 = 4'

(fig1_alt_long_summary_added %>%
	dplyr::filter(copyRate %in% c(0, 0.25, 0.5)) %>%
	dplyr::filter(conformityExp %in% c(1, 4)) %>%
	dplyr::filter(invTemperature %in% c(3,5,7)) %>%
	ggplot() +
	geom_point(aes(hot_stove_suceptibility, 1-mean_proportionSafeChoice, colour=copyRate_factor)) +
	stat_function(data=data.frame(X=c(0,8),invTemperature_factor='β = 3'), fun=Pr_when_beta, n = 1001, args=list(beta=3)) +
	stat_function(data=data.frame(X=c(0,8),invTemperature_factor='β = 5'), fun=Pr_when_beta, n = 1001, args=list(beta=5)) +
	stat_function(data=data.frame(X=c(0,8),invTemperature_factor='β = 7'), fun=Pr_when_beta, n = 1001, args=list(beta=7)) +
	geom_vline(xintercept=2, linetype='dashed')+
	geom_hline(yintercept=0.5, linetype='dashed')+
	facet_grid(invTemperature_factor ~ conformityExp_factor) +
	scale_colour_viridis_d(expression(sigma))+
	labs(
		y='Probability of choosing\nthe risky alternative',
		x=expression(alpha * (beta + 1))
	)+
	xlim(c(0,8))+
	myTheme_Helvetica()+
	NULL -> fig1_alt_long_plot
)

ggsave(file = "~/Dropbox/wataru/papers/RiskySocialLearning/draft/overleaf/figs/fig1_alt_long_plot.pdf", plot = fig1_alt_long_plot, dpi = 300, width = 6, height = 4.5, device = cairo_pdf)
